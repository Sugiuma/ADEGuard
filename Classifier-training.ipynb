{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d77556-7d77-4b74-a42d-b03fa5577df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 0Ô∏è‚É£ Imports\n",
    "# -----------------------------\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£ Load CSV + Snorkel weak labels\n",
    "# -----------------------------\n",
    "df = pd.read_csv(\"dataset_with_entities_and_weaklabels.csv\")\n",
    "\n",
    "# Keep only rows with valid weak label\n",
    "df = df[df[\"weak_label_id\"] != -1].copy()\n",
    "df[\"severity_id\"] = df[\"weak_label_id\"].astype(int)\n",
    "\n",
    "# Optional: check counts\n",
    "print(df[\"severity_id\"].value_counts())\n",
    "\n",
    "# -----------------------------\n",
    "# 2Ô∏è‚É£ Save JSONL for classifier\n",
    "# -----------------------------\n",
    "def save_jsonl(filename, df):\n",
    "    records = df.to_dict(orient=\"records\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for rec in records:\n",
    "            f.write(json.dumps({\n",
    "                \"tokens\": rec[\"symptom_combined\"].split(),  # or SYMPTOM_TEXT\n",
    "                \"severity_id\": rec[\"severity_id\"]\n",
    "            }) + \"\\n\")\n",
    "\n",
    "# Train/val/test split\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df[\"severity_id\"])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[\"severity_id\"])\n",
    "\n",
    "save_jsonl(\"train.jsonl\", train_df)\n",
    "save_jsonl(\"val.jsonl\", val_df)\n",
    "save_jsonl(\"test.jsonl\", test_df)\n",
    "\n",
    "print(\"‚úÖ JSONL splits saved.\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3Ô∏è‚É£ Prepare Hugging Face Dataset\n",
    "# -----------------------------\n",
    "def prepare_severity_dataset(jsonl_file):\n",
    "    texts, labels = [], []\n",
    "    with open(jsonl_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            item = json.loads(line)\n",
    "            text = \" \".join(item[\"tokens\"])\n",
    "            severity = item[\"severity_id\"]\n",
    "            if severity in [0,1,2]:\n",
    "                texts.append(text)\n",
    "                labels.append(severity)\n",
    "    return Dataset.from_dict({\"text\": texts, \"label\": labels})\n",
    "\n",
    "train_ds = prepare_severity_dataset(\"train.jsonl\")\n",
    "val_ds   = prepare_severity_dataset(\"val.jsonl\")\n",
    "test_ds  = prepare_severity_dataset(\"test.jsonl\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4Ô∏è‚É£ Tokenization\n",
    "# -----------------------------\n",
    "sev_tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-base-cased-v1.1\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return sev_tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "train_ds = train_ds.map(tokenize_fn, batched=True)\n",
    "val_ds   = val_ds.map(tokenize_fn, batched=True)\n",
    "test_ds  = test_ds.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Remove raw text column\n",
    "train_ds = train_ds.remove_columns([\"text\"])\n",
    "val_ds   = val_ds.remove_columns([\"text\"])\n",
    "test_ds  = test_ds.remove_columns([\"text\"])\n",
    "\n",
    "# -----------------------------\n",
    "# 5Ô∏è‚É£ Define model\n",
    "# -----------------------------\n",
    "num_labels = 3  # MILD, MODERATE, SEVERE\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"dmis-lab/biobert-base-cased-v1.1\",\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6Ô∏è‚É£ Metrics\n",
    "# -----------------------------\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    return {\"accuracy\": acc, \"f1\": f1}\n",
    "\n",
    "# -----------------------------\n",
    "# 7Ô∏è‚É£ TrainingArguments\n",
    "# -----------------------------\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bioBERT_severity_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    save_total_limit=2\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 8Ô∏è‚É£ Trainer\n",
    "# -----------------------------\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,\n",
    "    tokenizer=sev_tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 9Ô∏è‚É£ Train\n",
    "# -----------------------------\n",
    "trainer.train()\n",
    "\n",
    "# -----------------------------\n",
    "# üîü Evaluate on test set\n",
    "# -----------------------------\n",
    "metrics = trainer.evaluate(test_ds)\n",
    "print(\"Test set metrics:\", metrics)\n",
    "\n",
    "# -----------------------------\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Save final model\n",
    "# -----------------------------\n",
    "trainer.save_model(\"./bioBERT_severity_model_final\")\n",
    "print(\"‚úÖ Model saved at ./bioBERT_severity_model_final\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
